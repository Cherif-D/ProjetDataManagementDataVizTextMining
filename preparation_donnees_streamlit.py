#!/usr/bin/env python
# coding: utf-8

# Phases de pr√©paration des donn√©es
# ===

# Importation des packages
# ---

# In[ ]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import os


# Lecture du fichier csv t√©l√©charg√© et informations sur la base de donn√©es
# --

# In[2]:


df = pd.read_csv("Data/donnees_financieres_300k_lignes.csv")
df


# In[3]:


print(df.head(10))  # Affiche les 5 premi√®res lignes


# In[4]:


df.columns


# In[5]:


df.index


# In[6]:


df.info()


# In[8]:


df['Date'] = pd.to_datetime(df['Date'])


# In[9]:


df.info()


# In[10]:


df['Ticker'] = df['Ticker'].astype('string')  # Conversion en string


# In[11]:


df.info()


# In[12]:


df.shape


# In[13]:


df.describe()  # Affiche les statistiques descriptives
#df.describe(include='all')  # Inclut les colonnes non num√©riques


# Analyse exploiratoire des donn√©es
# --

# In[14]:


#V√©rification des doublons

print("üîç Nombre de doublons :", df.duplicated().sum()) 



# In[15]:


# v√©rification de la plage de dates et de la coh√©rence des dates
# Date minimale et maximale globale du DataFrame
print(f"Date minimale globale : {df['Date'].min()}")
print(f"Date maximale globale : {df['Date'].max()}")

# Date minimale et maximale par ticker (tr√®s utile pour voir les d√©buts d'historique)
print("\nDates minimales et maximales par Ticker :")
print(df.groupby('Ticker')['Date'].agg(['min', 'max']))



# Normal comme r√©sultat car la pr√©mi√®re date de trading est le 03/01/2000 m√™me si le t√©l√©chargement commence le 01/01/2000


# In[16]:


# Compter le nombre de jours uniques pour chaque ticker
print("\nNombre de jours d'historique par Ticker :")
print(df.groupby('Ticker')['Date'].nunique().sort_values(ascending=False))


# In[17]:


# V√©rification des valeurs manquantes
# Analyse compl√®te des NA
missing_data = df.isnull().sum().sort_values(ascending=False)
missing_percent = (df.isnull().mean()*100).round(2)

print("Valeurs manquantes par colonne:")
print(missing_data[missing_data > 0])  # Affiche seulement les colonnes avec NA

# Visualisation des NA (avec une heatmap)


plt.figure(figsize=(10,6))
sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
plt.title("Carte des valeurs manquantes")
plt.show()
#sortie du graphique : Une heatmap o√π :

 #Les lignes repr√©sentent les observations

# Les colonnes repr√©sentent les variables (Date, Ticker, Prix)

# Les zones jaunes/vertes montrent les valeurs manquantes (Prix)

# Les zones violettes montrent les valeurs pr√©sentes


# In[18]:


# 1. Calcul des valeurs manquantes par ticker
missing_by_ticker = df.groupby('Ticker')['Prix'].agg([
    ('total_manquants', lambda x: x.isnull().sum()),
    ('total_observations', 'count'),
    ('pourcentage_manquants', lambda x: round(x.isnull().mean()*100, 2))
])

# 2. Filtrage et tri des tickers avec des NA
missing_tickers = missing_by_ticker[missing_by_ticker['total_manquants'] > 0]\
                  .sort_values('pourcentage_manquants', ascending=False)

# 3. Affichage des r√©sultats
print("üìä Analyse d√©taill√©e des valeurs manquantes par actif:")
print(f"‚Ä¢ {len(missing_tickers)} actifs sur {len(missing_by_ticker)} contiennent des NA")
print(f"‚Ä¢ Moyenne de NA par actif: {missing_tickers['pourcentage_manquants'].mean():.1f}%")
print("\nTop 10 des actifs avec le plus de valeurs manquantes:")
print(missing_tickers.head(10))

# 4. Visualisation
plt.figure(figsize=(12, 8))

# Histogramme des pourcentages de NA
plt.subplot(2, 1, 1)
sns.histplot(data=missing_tickers, x='pourcentage_manquants', bins=30)
plt.title('Distribution des pourcentages de valeurs manquantes par actif')
plt.xlabel('% de valeurs manquantes')
plt.ylabel('Nombre d\'actifs')

# Top 15 des actifs avec le plus de NA
plt.subplot(2, 1, 2)
missing_tickers.head(15)['pourcentage_manquants'].plot(kind='barh', color='salmon')
plt.title('Top 15 des actifs avec le plus de valeurs manquantes')
plt.xlabel('% de valeurs manquantes')
plt.tight_layout()
plt.show()

# 5. Analyse temporelle des NA (exemple pour un ticker probl√©matique)
if not missing_tickers.empty:
    sample_ticker = missing_tickers.index[0]
    ticker_data = df[df['Ticker'] == sample_ticker][['Date', 'Prix']].set_index('Date')
    
    print(f"\nüîç Analyse temporelle pour {sample_ticker}:")
    plt.figure(figsize=(12, 4))
    plt.plot(ticker_data['Prix'], label='Prix')
    plt.scatter(ticker_data[ticker_data['Prix'].isnull()].index, 
                [0]*len(ticker_data[ticker_data['Prix'].isnull()]), 
                color='red', label='Valeurs manquantes')
    plt.title(f'Emplacement des NA pour {sample_ticker}')
    plt.legend()
    plt.show()


# Pic 0-20% :
# 
# 35 actifs ont peu de NA (<20%)
# 
# Ce sont les actifs les plus fiables (ex: grandes entreprises cot√©es depuis longtemps)
# 
# Queue droite :
# 
# Quelques outliers (5-10 actifs) ont 40-60% de NA
# 
# Probablement :
# 
# Cryptomonnaies r√©centes
# 
# Entreprises ayant fait faillite.

# In[19]:


# reserver aux autres qui feront la visualisation
# Premi√®re et derni√®re date disponible par actif
coverage = df.groupby('Ticker')['Date'].agg(['min', 'max', 'count'])
coverage['duration'] = coverage['max'] - coverage['min']
print("\nCouverture temporelle par actif:")
print(coverage.sort_values('duration', ascending=False))

# Visualisation de la couverture (n√©cessite plotly pour une meilleure interactivit√©)
fig = px.scatter(coverage.reset_index(), 
                 x='min', y='Ticker', 
                 size='count',
                 title="Couverture temporelle des actifs")
import plotly.io as pio
pio.renderers.default = 'browser'
fig.show()


# Statistiques descriptives avant le traitement des valeurs manquantes
# ----

# In[20]:


print("üìä Statistiques AVANT nettoyage des valeurs manquantes :")
print(df['Prix'].describe())

print("\nüîç Nombre de valeurs manquantes :", df['Prix'].isna().sum())
print(f"üîé Nombre de lignes : {df.shape[0]}")
print(f"üîé Nombre d'actifs : {df['Ticker'].nunique()}")


# Phases de nettoyage des valeurs manquantes des doublons et justifications
# ===

# Les doublons n‚Äôont aucun int√©r√™t analytique ici. Chaque ligne repr√©sente une observation unique d‚Äôun prix √† une date pour un actif donn√©. Nous les supprimons pour garantir l‚Äôunicit√© des observations.

# In[21]:


# üîé 1. SUPPRESSION DES DOUBLONS
print("Nombre de doublons avant suppression :", df.duplicated().sum())
df.drop_duplicates(inplace=True)
print("‚úîÔ∏è Doublons supprim√©s. Nombre de lignes restantes :", df.shape[0])


# 
# Tous les actifs ne sont pas comparables :
# 
# Les cryptomonnaies sont tr√®s r√©centes ‚Üí historique tr√®s court
# 
# Les ETF peuvent aussi avoir √©t√© cr√©√©s tardivement
# 
# Les actions comme TSLA ou META n‚Äô√©taient pas cot√©es avant les ann√©es 2010
# 
# üîç Appliquer un seuil unique (ex. 30%) serait trop rigide et √©liminerait des actifs pertinents
# 
# ‚úÖ On adopte une strat√©gie diff√©renci√©e selon le type d‚Äôactif :
# 
# Type d‚Äôactif	Seuil de suppression
# Crypto	> 50 % de NaN
# Action	> 60 % de NaN
# ETF	> 60 % de NaN

# In[28]:


# Cr√©e un dictionnaire manuel pour mapper les tickers √† leur cat√©gorie d‚Äôactif
type_map = {
    # üî∂ Cryptomonnaies
    'BTC-USD': 'Crypto',     # Bitcoin
    'ETH-USD': 'Crypto',     # Ethereum
    'XRP-USD': 'Crypto',     # Ripple
    'LTC-USD': 'Crypto',     # Litecoin

    # üî∑ ETF
    'SPY': 'ETF',            # S&P 500 ETF
    'QQQ': 'ETF',            # Nasdaq-100 ETF
    'DIA': 'ETF',            # Dow Jones ETF
    'TLT': 'ETF',            # Treasury Bonds ETF (20+ years)
    'IEF': 'ETF',            # Treasury Bonds ETF (7‚Äì10 years)
    'GLD': 'ETF',            # Gold ETF
    'SLV': 'ETF',            # Silver ETF

    # üîµ Actions technologiques / GAFAM
    'AAPL': 'Action',        # Apple ‚Äì Technologie
    'MSFT': 'Action',        # Microsoft ‚Äì Technologie
    'GOOGL': 'Action',       # Alphabet/Google ‚Äì Technologie
    'META': 'Action',        # Meta (Facebook) ‚Äì R√©seaux sociaux
    'AMZN': 'Action',        # Amazon ‚Äì E-commerce & Cloud
    'NFLX': 'Action',        # Netflix ‚Äì Streaming
    'NVDA': 'Action',        # Nvidia ‚Äì Semi-conducteurs
    'ADBE': 'Action',        # Adobe ‚Äì Logiciels

    # üîµ Actions financi√®res
    'JPM': 'Action',         # JPMorgan Chase ‚Äì Banque
    'BAC': 'Action',         # Bank of America ‚Äì Banque
    'WFC': 'Action',         # Wells Fargo ‚Äì Banque
    'GS': 'Action',          # Goldman Sachs ‚Äì Banque d'investissement
    'MS': 'Action',          # Morgan Stanley ‚Äì Banque d'investissement
    'V': 'Action',           # Visa ‚Äì Paiements
    'MA': 'Action',          # Mastercard ‚Äì Paiements

    # üîµ Actions industrielles
    'GE': 'Action',          # General Electric ‚Äì Industrie
    'CAT': 'Action',         # Caterpillar ‚Äì Machines industrielles
    'BA': 'Action',          # Boeing ‚Äì A√©ronautique

    # üîµ Actions √©nergie
    'XOM': 'Action',         # ExxonMobil ‚Äì P√©trole
    'CVX': 'Action',         # Chevron ‚Äì P√©trole

    # üîµ Actions sant√©
    'JNJ': 'Action',         # Johnson & Johnson ‚Äì Sant√©
    'LLY': 'Action',         # Eli Lilly ‚Äì Sant√©
    'PFE': 'Action',         # Pfizer ‚Äì Pharmaceutique
    'MRK': 'Action',         # Merck & Co ‚Äì Pharmaceutique
    'ABBV': 'Action',        # AbbVie ‚Äì Biotechnologie
    'UNH': 'Action',         # UnitedHealth ‚Äì Assurance sant√©
    'TMO': 'Action',         # Thermo Fisher ‚Äì Biotechnologie

    # üîµ Actions consommation
    'WMT': 'Action',         # Walmart ‚Äì Distribution
    'HD': 'Action',          # Home Depot ‚Äì Bricolage
    'COST': 'Action',        # Costco ‚Äì Grande distribution
    'DIS': 'Action',         # Disney ‚Äì Divertissement
    'NKE': 'Action',         # Nike ‚Äì √âquipement sportif
    'KO': 'Action',          # Coca-Cola ‚Äì Boissons
    'PEP': 'Action',         # PepsiCo ‚Äì Boissons et snacks
    'PG': 'Action',          # Procter & Gamble ‚Äì Produits m√©nagers

    # üîµ Actions t√©l√©coms & communication
    'VZ': 'Action',          # Verizon ‚Äì T√©l√©com
    'TMUS': 'Action',        # T-Mobile US ‚Äì T√©l√©com
    'CMCSA': 'Action',       # Comcast ‚Äì T√©l√©com/C√¢ble

    # üîµ Actions technologiques (divers)
    'CRM': 'Action',         # Salesforce ‚Äì Logiciels
    'CSCO': 'Action',        # Cisco ‚Äì R√©seaux
    'TXN': 'Action',         # Texas Instruments ‚Äì Semi-conducteurs
    'AVGO': 'Action',        # Broadcom ‚Äì Semi-conducteurs
    'DHR': 'Action',          # Danaher ‚Äì Sant√©/Instrumentation
    # üîµ Actions automobiles
    'TSLA': 'Action',     # Tesla ‚Äì V√©hicules √©lectriques / Technologie

}



# Pour ne pas modifier le DataFrame original (df), on cr√©e une copie ind√©pendante.
# On y ajoutera la colonne Type_actif pour appliquer notre strat√©gie de filtrage.

# In[29]:


# On travaille sur une copie pour pr√©server le DataFrame original
df_copy = df.copy()

# Ajoute une colonne Type_actif gr√¢ce au mapping manuel
df_copy['Type_actif'] = df_copy['Ticker'].map(type_map)


# In[30]:


# Extraction des tickers dont le Type_actif est manquant
tickers_manquants = df_copy[df_copy['Type_actif'].isna()]['Ticker'].unique()

# Affichage du r√©sultat
print(f"‚ö†Ô∏è {len(tickers_manquants)} ticker(s) n'ont pas encore de Type_actif d√©fini :")
print(sorted(tickers_manquants))


# In[31]:


# Conversion de la colonne 'Date' au format datetime
df_copy['Date'] = pd.to_datetime(df_copy['Date'], errors='coerce')

# Conversion des colonnes cat√©gorielles en string propre
df_copy['Ticker'] = df_copy['Ticker'].astype('string')
df_copy['Type_actif'] = df_copy['Type_actif'].astype('string')

print(df_copy.info())


# In[32]:


df_copy


# On r√©cup√®re les r√©sultats de l‚Äôanalyse des valeurs manquantes (missing_tickers)
# On y ajoute aussi le type d‚Äôactif pour pouvoir filtrer selon les r√®gles d√©finies.

# In[33]:


df_na = missing_tickers.copy()
df_na['Type_actif'] = df_na.index.map(type_map)


# Application de r√®gles sp√©cifiques par type d‚Äôactif :
# 
# Suppression des cryptos avec >50 % de NaN
# 
# Suppression des actions et ETF avec >60 % de NaN

# In[34]:


# D√©finition des conditions selon le type d‚Äôactif
condition_cryptos = (df_na['Type_actif'] == 'Crypto') & (df_na['pourcentage_manquants'] > 50)
condition_actions = (df_na['Type_actif'] == 'Action') & (df_na['pourcentage_manquants'] > 60)
condition_etf = (df_na['Type_actif'] == 'ETF') & (df_na['pourcentage_manquants'] > 60)

# Liste finale des tickers √† exclure
tickers_exclus = df_na[condition_cryptos | condition_actions | condition_etf]
actifs_supprimes = tickers_exclus.index.tolist()


# On supprime uniquement les lignes correspondant aux tickers trop incomplets,
# et on conserve les autres dans une nouvelle version propre : df_clean.

# In[35]:


# Cr√©ation du DataFrame nettoy√© (tickers filtr√©s)
df_clean = df_copy[~df_copy['Ticker'].isin(actifs_supprimes)].copy()


# On applique un double remplissage (ffill() + bfill()) :
# 
# ffill() pour remplir les trous par la derni√®re valeur connue (logique financi√®re)
# 
# bfill() pour compl√©ter les valeurs initiales manquantes si aucune valeur connue avant

# In[36]:


# Tri n√©cessaire avant le remplissage temporel
df_clean.sort_values(by=['Ticker', 'Date'], inplace=True)

# Remplissage par actif
df_clean['Prix'] = df_clean.groupby('Ticker')['Prix'].transform(lambda x: x.ffill().bfill())


# In[38]:


print("üßº Actifs supprim√©s selon strat√©gie personnalis√©e :\n")
print(tickers_exclus[['Type_actif', 'total_manquants', 'pourcentage_manquants']])

print(f"\n‚ùó Nombre total d'actifs supprim√©s : {len(actifs_supprimes)}")
print("üîª Tickeurs supprim√©s :")
print(", ".join(actifs_supprimes))

print(f"\n‚úÖ Donn√©es pr√™tes pour analyse avec {df_clean.shape[0]} lignes et {df_clean['Ticker'].nunique()} actifs conserv√©s.")


# In[39]:


print("üìä Statistiques APR√àS nettoyage des valeurs manquantes :")
print(df_clean['Prix'].describe())

print("\nüßº Nombre de valeurs manquantes restantes :", df_clean['Prix'].isna().sum())
print(f"‚úÖ Nombre de lignes restantes : {df_clean.shape[0]}")
print(f"‚úÖ Nombre d'actifs conserv√©s : {df_clean['Ticker'].nunique()}")


# In[40]:


fig, ax = plt.subplots(1, 2, figsize=(15, 5))
sns.boxplot(data=df, x='Prix', ax=ax[0]).set_title("AVANT nettoyage")
sns.boxplot(data=df_clean, x='Prix', ax=ax[1]).set_title("APR√àS nettoyage")
plt.show()


# Difficile √† interpr√©ter 

# In[41]:


# --- Visualisation des Distributions de Prix (AVANT vs APR√àS nettoyage) ---
print("\n--- Visualisation des Distributions de Prix (AVANT vs APR√àS nettoyage) avec KDE et √©chelle Log ---")

# Utilisez np.log1p sur les prix, en g√©rant les NaN pour la visualisation
# (les NaN sont d√©j√† g√©r√©s dans df_clean, mais on les retire pour df pour la visualisation)
prix_avant_log = np.log1p(df['Prix'].dropna())
prix_apres_log = np.log1p(df_clean['Prix'].dropna())

plt.figure(figsize=(12, 7))

# Superposition des deux distributions pour une comparaison directe
sns.kdeplot(prix_avant_log, fill=True, color='skyblue', label='AVANT Nettoyage', alpha=0.6)
sns.kdeplot(prix_apres_log, fill=True, color='lightgreen', label='APR√àS Nettoyage', alpha=0.6)

plt.title('Comparaison des Distributions de Prix (√©chelle log) - AVANT vs APR√àS Nettoyage')
plt.xlabel('Log(1 + Prix)')
plt.ylabel('Densit√©')
plt.legend()
plt.grid(True)
plt.show()

print("\nCommentaire sur le graphique KDE :")
print("Ce graphique de densit√© en √©chelle logarithmique permet de mieux visualiser la forme de la distribution des prix avant et apr√®s le nettoyage.")
print("La transformation logarithmique compresse les grandes valeurs et √©tire les petites, rendant les diff√©rences subtiles plus apparentes.")
print("Vous pouvez observer si la forme de la distribution a chang√©, si des modes ont disparu ou sont apparus, ou si la dispersion s'est modifi√©e suite √† la suppression des actifs avec de nombreux NaN.")


# Commentaire sur la Comparaison des Distributions de Prix (AVANT vs APR√àS Nettoyage)
# Ce graphique montre comment les prix de nos actifs sont r√©partis, avant et apr√®s que nous ayons nettoy√© les donn√©es. Comme les prix peuvent √™tre tr√®s diff√©rents (certains tr√®s petits, d'autres tr√®s grands comme le Bitcoin), nous utilisons une √©chelle sp√©ciale appel√©e "√©chelle logarithmique" sur l'axe des prix. Cela nous permet de bien voir toutes les valeurs, petites et grandes.
# 
# Courbe bleue ("AVANT Nettoyage") : Elle repr√©sente la r√©partition des prix bruts, tels qu'ils √©taient quand nous avons t√©l√©charg√© les donn√©es.
# Courbe verte ("APR√àS Nettoyage") : Elle repr√©sente la r√©partition des prix apr√®s que nous ayons corrig√© les donn√©es manquantes (par exemple, en supprimant les actifs qui n'existaient pas au d√©but de notre p√©riode d'√©tude parce qu'ils avaient trop de donn√©es manquantes √† ces dates).
# Ce que nous pouvons observer :
# 
# Forme G√©n√©rale Similaire : Les deux courbes se ressemblent beaucoup. Cela veut dire que notre nettoyage n'a pas radicalement chang√© la fa√ßon dont les prix sont r√©partis dans l'ensemble. La majorit√© des prix se regroupent toujours autour des m√™mes valeurs (autour de 3 √† 4 sur l'√©chelle log, ce qui correspond √† des prix moyens apr√®s transformation).
# L√©g√®res diff√©rences :
# La courbe verte ("APR√àS Nettoyage") est un peu plus concentr√©e et a un pic l√©g√®rement plus √©lev√© autour de la valeur moyenne. Cela est probablement d√ª au fait que nous avons supprim√© certains actifs qui avaient beaucoup de donn√©es manquantes, surtout ceux qui n'existaient que sur une courte p√©riode ou qui avaient des prix tr√®s bas au d√©but de leur historique. En les retirant, les donn√©es restantes sont un peu plus "homog√®nes" et mieux repr√©sent√©es.
# On peut voir que la courbe bleue a une "queue" un peu plus large √† l'extr√™me gauche (autour de 0-1 sur l'√©chelle log) qui est moins pr√©sente dans la courbe verte. Cela pourrait indiquer que le nettoyage a permis de retirer des prix tr√®s bas associ√©s √† des donn√©es incompl√®tes ou peu repr√©sentatives.
# En r√©sum√©, le nettoyage des donn√©es n'a pas boulevers√© la distribution g√©n√©rale des prix, mais il l'a rendue un peu plus pr√©cise et coh√©rente en enlevant les actifs qui rendaient l'analyse plus difficile √† cause de leurs nombreuses valeurs manquantes.

# In[ ]:


# Nom du dossier pour sauvegarder le fichier
output_dir = 'Data'

# Nom du fichier CSV pour les donn√©es nettoy√©es
output_filename = 'donnees_financieres_clean.csv'

# Chemin complet vers le fichier de sortie
output_path = os.path.join(output_dir, output_filename)

# V√©rifier si le dossier existe, sinon le cr√©er
# exist_ok=True √©vite une erreur si le dossier existe d√©j√†
os.makedirs(output_dir, exist_ok=True)

# Enregistrer le DataFrame df_clean en tant que fichier CSV
# index=True est important pour inclure l'index (qui contient les dates) dans le fichier CSV
df_clean.to_csv(output_path, index=True)

print(f"Le fichier '{output_filename}' a √©t√© enregistr√© avec succ√®s dans le dossier '{output_dir}'.")
print(f"Chemin complet : {os.path.abspath(output_path)}")


# Phase de cr√©ation de variables pertinentes pour l‚Äôanalyse & Streamlit
# ===

# Cr√©er des variables qui :
# 
# üß≠ permettent une exploration intuitive dans Streamlit
# 
# üìä facilitent les visualisations dynamiques (filtres, regroupements)
# 
# üîç aident √† r√©v√©ler des tendances et anomalies sur les actifs

# Variable secteur : Le type d‚Äôactif (Action, Crypto, ETF) est trop g√©n√©ral pour les analyses comparatives.
# On cr√©e une variable Secteur pour regrouper les entreprises selon leur activit√© (technologie, sant√©, √©nergie...).
# Cela permettra :
# 
# de filtrer dans Streamlit selon le secteur,
# 
# de visualiser la performance moyenne ou la volatilit√© par secteur,
# 
# d‚Äôidentifier des secteurs plus risqu√©s ou plus stables.

# In[71]:


# üß¨ 1. Copier le DataFrame de base pour enrichissement
df_enrichi = df_clean.copy()


# üß≠ 2. Appliquer le mapping secteur √† partir du dictionnaire
secteur_map = {
    'AAPL': 'Technologie', 'MSFT': 'Technologie', 'GOOGL': 'Technologie', 'META': 'Technologie', 'ADBE': 'Technologie', 'CRM': 'Technologie',
    'AMZN': 'E-commerce', 'NFLX': 'Divertissement', 'DIS': 'Divertissement',
    'NVDA': 'Semi-conducteurs', 'AVGO': 'Semi-conducteurs', 'TXN': 'Semi-conducteurs', 'CSCO': 'Technologie r√©seaux',
    'TSLA': 'Automobile', 'CAT': 'Industrie', 'GE': 'Industrie', 'BA': 'A√©ronautique', 'DHR': 'Mat√©riel m√©dical',
    'JNJ': 'Sant√©', 'PFE': 'Sant√©', 'MRK': 'Sant√©', 'LLY': 'Sant√©', 'ABBV': 'Biotechnologie', 'TMO': 'Biotechnologie', 'UNH': 'Assurance sant√©',
    'JPM': 'Banque', 'BAC': 'Banque', 'WFC': 'Banque', 'GS': 'Banque', 'MS': 'Banque', 'V': 'Paiements', 'MA': 'Paiements',
    'WMT': 'Grande distribution', 'COST': 'Grande distribution', 'HD': 'Bricolage',
    'NKE': 'Consommation', 'PG': 'Consommation', 'KO': 'Consommation', 'PEP': 'Consommation',
    'SPY': 'ETF', 'QQQ': 'ETF', 'DIA': 'ETF', 'TLT': 'ETF', 'IEF': 'ETF', 'GLD': 'ETF', 'SLV': 'ETF',
    'BTC-USD': 'Crypto', 'ETH-USD': 'Crypto', 'XRP-USD': 'Crypto', 'LTC-USD': 'Crypto',
    'CMCSA': 'T√©l√©com', 'VZ': 'T√©l√©com', 'TMUS': 'T√©l√©com',
    'CVX': '√ânergie', 'XOM': '√ânergie'
}

df_enrichi['Secteur'] = df_enrichi['Ticker'].map(secteur_map).astype('string')


# In[72]:


# üîé 3. V√©rifier les valeurs manquantes dans la colonne 'Secteur'
secteurs_vides = df_enrichi[df_enrichi['Secteur'].isna()]['Ticker'].unique()

if len(secteurs_vides) == 0:
    print("‚úÖ Tous les tickers ont un secteur attribu√©.")
else:
    print(f"‚ö†Ô∏è {len(secteurs_vides)} ticker(s) n'ont pas de secteur d√©fini :")
    print(sorted(secteurs_vides))


# Variable "Rendement" (Returns)
# 
# Objectif : Calculer le rendement quotidien pour analyser la performance.
# D√©finition : variation relative du prix entre deux jours cons√©cutifs pour un m√™me actif

# In[73]:


# üßÆ 4. Calculer le rendement quotidien en pourcentage
df_enrichi.sort_values(['Ticker', 'Date'], inplace=True)  # Tri n√©cessaire
df_enrichi['Rendement'] = df_enrichi.groupby('Ticker')['Prix'].pct_change() * 100  # En %

print("‚úÖ Variable 'Rendement' (en %) cr√©√©e avec succ√®s.")


# In[74]:


df_enrichi


#  V√©rifier le nombre de valeurs manquantes dans Rendement
# Donc, nombre valeurs manquantes attendu ‚âà nombre de tickers dans le dataset.

# In[75]:


nb_na_rendement = df_enrichi['Rendement'].isna().sum()
nb_total = len(df_enrichi)

print(f"üìä Nombre de rendements manquants : {nb_na_rendement} / {nb_total} lignes")

pourcentage_na = (nb_na_rendement / nb_total) * 100
print(f"üîé Pourcentage de valeurs manquantes dans 'Rendement' : {pourcentage_na:.2f}%")


# Cr√©ation de la variable Ann√©e
# üìå Justification :
# Extraire des composantes temporelles permet d'analyser les donn√©es √† diff√©rentes √©chelles.
# La colonne Ann√©e est essentielle pour :
# 
# √âtudier l‚Äô√©volution des performances ou des risques ann√©e par ann√©e ;
# 
# Construire des moyennes ou courbes annuelles dans Streamlit ;
# 
# Appliquer des filtres temporels dans des tableaux ou des graphiques.

# In[76]:


# üïì S'assurer que la colonne 'Date' est bien au format datetime
df_enrichi['Date'] = pd.to_datetime(df_enrichi['Date'], errors='coerce')

# üìÖ Cr√©ation de la colonne 'Ann√©e'
df_enrichi['Ann√©e'] = df_enrichi['Date'].dt.year

# ‚úÖ V√©rification que toutes les ann√©es sont bien remplies
nb_na_annee = df_enrichi['Ann√©e'].isna().sum()

if nb_na_annee == 0:
    print("‚úÖ La colonne 'Ann√©e' a √©t√© ajout√©e avec succ√®s, sans valeur manquante.")
else:
    print(f"‚ö†Ô∏è Attention : {nb_na_annee} valeur(s) manquante(s) dans la colonne 'Ann√©e'.")


# In[77]:


df_enrichi


# Cr√©ation de deux colonnes de volatilit√©
# 
# La volatilit√© 30 jours est utile pour d√©tecter rapidement les p√©riodes de forte instabilit√©.
# 
# La version annualis√©e est plus facilement comparable entre actifs, car c‚Äôest une norme du secteur financier.
# 
# La volatilit√© quotidienne moyenne permet de :comparer le risque moyen historique des actifs (Crypto vs Actions vs ETF),g√©n√©rer un classement de stabilit√© des actifs,tracer un barplot par secteur ou par capitalisation dans Streamlit.

# In[78]:


# ‚úÖ 1. Volatilit√© glissante sur 30 jours (en % journalier)
df_enrichi['Volatilit√©_30j'] = df_enrichi.groupby('Ticker')['Rendement'].transform(
    lambda x: x.rolling(window=30).std()
)

# ‚úÖ 2. Volatilit√© annualis√©e sur 30 jours
df_enrichi['Volatilit√©_30j_annualis√©e'] = df_enrichi['Volatilit√©_30j'] * np.sqrt(252)

# ‚úÖ 3. Volatilit√© quotidienne globale par actif
volatilite_quotidienne = df_enrichi.groupby('Ticker')['Rendement'].std()

# ‚ûï On la r√©injecte dans le DataFrame principal (m√™me valeur r√©p√©t√©e par ticker)
df_enrichi['Volatilit√©_quotidienne'] = df_enrichi['Ticker'].map(volatilite_quotidienne)

print("‚úÖ Variables 'Volatilit√©_30j', 'Volatilit√©_30j_annualis√©e' et 'Volatilit√©_quotidienne' ajout√©es avec succ√®s.")


# In[79]:


df_enrichi


# Variable : Performance relative actif vs benchmark
# 
#  Objectif :
# Comparer la performance quotidienne d‚Äôun actif √† un benchmark de march√© adapt√© pour voir si l‚Äôactif surperforme ou sous-performe en pourcentage par rapport au march√©.
# 
# Type d‚Äôactif / Secteur	Benchmark recommand√©
# üìà Actions (US large-cap)	SPY
# üíª Tech / Nasdaq	QQQ
# üõ¢Ô∏è √ânergie / Industrie	DIA
# üè¶ Obligations longues	TLT
# üí∞ Obligations courtes	IEF
# ü™ô Crypto-monnaies	BTC-USD (optionnel)
# ü•á Or / Argent / mati√®res	GLD ou SLV
# 
# Cr√©er une variable Performance_vs_Benchmark dynamique, qui :
# 
# üîÅ Compare chaque actif √† son benchmark adapt√© (et pas toujours SPY),
# 
# üéØ Donne plus de sens aux classements dans Streamlit (une action tech compar√©e au Nasdaq, pas au Dow Jones !).

# In[80]:


# üìä Association des actifs √† leur benchmark logique
benchmark_map = {
    # Actions Tech ‚Üí QQQ
    'AAPL': 'QQQ', 'MSFT': 'QQQ', 'GOOGL': 'QQQ', 'META': 'QQQ', 'AMZN': 'QQQ', 'NVDA': 'QQQ', 'ADBE': 'QQQ', 'CRM': 'QQQ',

    # Actions industrielles ou Dow Jones ‚Üí DIA
    'BA': 'DIA', 'CAT': 'DIA', 'GE': 'DIA',

    # Sant√© & large-cap divers ‚Üí SPY
    'JNJ': 'SPY', 'PFE': 'SPY', 'MRK': 'SPY', 'LLY': 'SPY', 'UNH': 'SPY', 'TMO': 'SPY',
    'PG': 'SPY', 'KO': 'SPY', 'PEP': 'SPY', 'DIS': 'SPY', 'NKE': 'SPY',
    'JPM': 'SPY', 'BAC': 'SPY', 'WFC': 'SPY', 'GS': 'SPY', 'MS': 'SPY', 'V': 'SPY', 'MA': 'SPY',
    'WMT': 'SPY', 'HD': 'SPY', 'COST': 'SPY', 'TSLA': 'SPY',

    # Obligations longues ‚Üí TLT
    'TLT': 'TLT',

    # Obligations courtes ‚Üí IEF
    'IEF': 'IEF',

    # Mati√®res premi√®res
    'GLD': 'GLD', 'SLV': 'SLV',

    # Crypto ‚Üí BTC comme r√©f√©rence
    'ETH-USD': 'BTC-USD', 'XRP-USD': 'BTC-USD', 'LTC-USD': 'BTC-USD',
    'BTC-USD': 'BTC-USD',

    # Benchmarks eux-m√™mes ‚Üí 100%
    'SPY': 'SPY', 'QQQ': 'QQQ', 'DIA': 'DIA',
    
     # üî¨ Biotechnologie / Sant√© ‚Üí SPY
    'ABBV': 'SPY',      # AbbVie ‚Äì Biotech large-cap

    # ‚öôÔ∏è Semi-conducteurs ‚Üí QQQ
    'AVGO': 'QQQ',      # Broadcom ‚Äì Tech / Semi
    'TXN': 'QQQ',       # Texas Instruments ‚Äì Semi
    'CSCO': 'QQQ',      # Cisco ‚Äì R√©seaux

    # üì∫ Communication / Streaming ‚Üí QQQ
    'NFLX': 'QQQ',      # Netflix ‚Äì Streaming (tech orient√©e consommation)

    # üè≠ Instrumentation / Sant√© ‚Üí SPY
    'DHR': 'SPY',       # Danaher ‚Äì Sant√©/Instrumentation

    # üì° T√©l√©coms ‚Üí DIA ou SPY selon pr√©f√©rence (ici SPY pour uniformit√©)
    'CMCSA': 'SPY',     # Comcast ‚Äì T√©l√©com
    'VZ': 'SPY',        # Verizon ‚Äì T√©l√©com
    'TMUS': 'SPY',      # T-Mobile ‚Äì T√©l√©com

    # üõ¢Ô∏è √ânergie ‚Üí DIA ou SPY (ici SPY pour large-cap classique)
    'XOM': 'SPY',       # ExxonMobil ‚Äì P√©trole
    'CVX': 'SPY',       # Chevron ‚Äì P√©trole
}


# In[81]:


# ‚ûï Ajout d'une colonne qui contient le benchmark adapt√© pour chaque ligne
df_enrichi['Benchmark'] = df_enrichi['Ticker'].map(benchmark_map)

# üìà Cr√©ation d‚Äôun dictionnaire avec les prix de chaque benchmark
benchmark_prices = df_enrichi[df_enrichi['Ticker'].isin(set(benchmark_map.values()))]
benchmark_series = benchmark_prices.pivot(index='Date', columns='Ticker', values='Prix')


# üßÆ Calcul dynamique par ligne
df_enrichi['Performance_vs_Benchmark'] = df_enrichi.apply(
    lambda row: (row['Prix'] / benchmark_series.loc[row['Date'], row['Benchmark']] * 100)
    if pd.notnull(row['Benchmark']) and row['Ticker'] != row['Benchmark']
    else 100,  # Benchmark lui-m√™me = 100
    axis=1
)

print("‚úÖ Variable 'Performance_vs_Benchmark' ajout√©e avec succ√®s.")



# In[82]:


# 1. Liste de tous les tickers de ton type_map
type_map_keys = set(type_map.keys())

# 2. Ton benchmark_map (doit √™tre d√©fini en amont comme dans la r√©ponse pr√©c√©dente)
benchmark_map_keys = set(benchmark_map.keys())

# 3. V√©rification des tickers manquants
tickers_sans_benchmark = sorted(type_map_keys - benchmark_map_keys)

if not tickers_sans_benchmark:
    print("‚úÖ Tous les tickers de type_map ont un benchmark d√©fini dans benchmark_map.")
else:
    print(f"‚ùå {len(tickers_sans_benchmark)} ticker(s) n'ont PAS de benchmark dans benchmark_map :")
    print(tickers_sans_benchmark)


# In[83]:


df_enrichi


# In[84]:


df_enrichi.columns


# In[85]:


# üîÑ Cr√©ation de la version finale pour Streamlit
dataframe_final_pret_pour_streamlit = df_enrichi.copy()

# üíæ Sauvegarde dans le dossier 'Data'
dataframe_final_pret_pour_streamlit.to_csv("Data/dataframe_final_pret_pour_streamlit.csv", index=False)

print("‚úÖ DataFrame sauvegard√© sous le nom 'dataframe_final_pret_pour_streamlit.csv' dans le dossier 'Data'.")

